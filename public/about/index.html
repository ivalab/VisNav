<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>IVALab Vision-Based Navigation Research</title>
<meta name="description" content="Research efforts related to vision-in-the-loop navigation.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<link rel="stylesheet" href="https://ivalab.github.io/VisNav/public/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="https://ivalab.github.io/VisNav/public/css/font-awesome.min.css">
<link rel="stylesheet" href="https://ivalab.github.io/VisNav/public/css/owl.carousel.css">
<link rel="stylesheet" href="https://ivalab.github.io/VisNav/public/css/owl.theme.css">


  <link href="https://ivalab.github.io/VisNav/public/css/style.ivalab.css" rel="stylesheet" id="theme-stylesheet">

 

  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  


<link href="https://ivalab.github.io/VisNav/public/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="https://ivalab.github.io/VisNav/public/img/favicon.png">

  <link href="https://ivalab.github.io/VisNav/public/about/index.xml" rel="alternate" type="application/rss+xml" title="VisNav Research" />


</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">

  <center style="margin-top:0px;margin-bottom:0px">
  <A HREF="https://ivalab.gatech.edu">
  <IMG SRC="https://ivalab.gatech.edu/ivalab.png">
  </A>
  </center>
    <a href="https://ivalab.github.io/VisNav/public">
    <h1 style="text-align:center;font-variant:small-caps" class="sidebar-heading">
    VisNav Research
    </h1>
    </a>
    
      <p class="sidebar-p">Research efforts related to egocentric, perception-space, visual navigation.</p>
    
    <ul class="sidebar-menu">
      
        <li><a href="https://ivalab.github.io/VisNav/public/about/">About</a></li>
      
        <li><a href="https://ivalab.github.io/VisNav/public/robots/">Robots</a></li>
      
        <li><a href="https://ivalab.github.io/VisNav/public/research/">Research</a></li>
      
    </ul>

    <p class="social">
  
  
  
  <a href="https://twitter.com/IVALab5" data-animate-hover="pulse" class="external twitter">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  
  <a href="https://github.com/ivalab" data-animate-hover="pulse" class="external">
    <i class="fa fa-github"></i>
  </a>
  
  
  <a href="https://github.com/ivaROS" data-animate-hover="pulse" class="external">
    <i class="fa fa-github"></i>
  </a>
  
  
  <a href="https://www.youtube.com/channel/UC-YbwwjK5GE1YjVolgNe5Kg/" data-animate-hover="pulse" class="external">
    <i class="fa fa-youtube"></i>
  </a>
  
  
</p>


  Select by Tags:<BR>
  
    <a href="https://ivalab.github.io/VisNav/public/tags/pips/"
    style="padding:0px 5px 0px 5px">PiPS</a>
  
    <a href="https://ivalab.github.io/VisNav/public/tags/stixel/"
    style="padding:0px 5px 0px 5px">Stixel</a>
  
  <a href="https://ivalab.github.io/VisNav/public" style="padding:0px 5px 0px 5px">All</a>

    <div class="support" style="padding-top:15px">
      <p class="credit">
        
          Research supported in part by National Science Foundation.  Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. 
        
      </p>
      <p class="support">
        <p class="credit">
      
        
          
            <a target="_blank" href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1849333&amp;HistoricalAwards=false">NSF &#35;1849333 </a><BR>
          
        
      
        </p>
      </p>
    </div>
    <div class="copyright">
      <p class="credit">
        
        Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>

</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="https://ivalab.github.io/VisNav/public">VisNav Research</a></h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>IVALab Vision-Based Navigation Research</h1>
         <p>Our research is inspired by earlier work on <em>controlled active vision</em>,
which examines the role of visual sensors in the feedback loop and
investigates how to design visual algorithms whose closed-loop
performance can be made to reproduce the properties of ideal feedback
control laws (in terms of their frequency domain responses or in
terms of some other established performance scoring function).
Importantly, it also looks at the role that online estimation and
adaptation plays in ensuring that the visual algorithm is operating
within the correct domain of attraction regarding its internal state
model.  Vision is an interesting modality because some processing must
occur in order to arrive at the feedback signal of interest. That
processing itself impacts the operation of the closed-loop system, so it
must be designed and managed to optimize performance.  Finally,
we take into consideration the real-time demands of the visual
algorithm. Doing so requires balancing numerical methods and algorithmic
implementation against solution complexity.  High performance is
meaningless if the algorithm cannot be implemented in real-time.</p>
<p>Here, we focus this perspective on the problem of collision-free
navigation through an unknown and uncontrolled space.  Most traditional
pipelines live at two extremes. On one side, there is sufficient
computational performance so that the idealized perceive, plan, act
approach can be realized. On the other side, more advanced data structures
and sensor throughput restrictions permit compute limited platforms to
achieve real-time computations. Issues arrive when looking at the
spectrum of computation, sensor throughput, and task complexity.
Algorithms designed for high performance machines do not scale down to
low performance computers.  Algorithms designed to optimize sensory data
access do not scale up to higher sensor throughputs. Our aim is to
establish a navigation algorithm with deterministic properties that can
be tuned based on the platform characteristics, or whose abilities can
be predicted based on the platform characteristics.</p>
<p>At the heart of this lies a <em>perception-space</em> approach to computation.
Rather than employ optimized but poorly-scaled data structures, and
rather than brute force the conversion of the data into a globally
consistent reference frame, we advocate for minimally processing the
sensory data and mapping the collision-avoidance problem from a spatial,
world representation to a visual, perceptual representation.  Don't map
the data to the robot's physical domain, but map the physical robot into
the sensor's egocentric domain. We call this egocentric approach
<em>Planning in Perception Space</em> or <em>PiPS</em>, and have just begun to
explore the benefits of such an intermediate representation for
generating computationally scalable navigation algorithms. Longer term,
we aim to also explore how these local methods can more robustly
integrate with global methods and exploit the multiple time-scales that
the different modules should operate at with regards to data
assimilation, solution rate, and other high-level navigation
meta-decisions.</p>
<h3 id="investigators-involved">Investigators Involved</h3>
<ul>
<li><a href="http://pvela.gatech.edu">Patricio A. Vela</a></li>
<li>Justin S. Smith (graduate)</li>
<li>Shiyu Feng (graduate)</li>
<li>Fanzhe Lyu (undergraduate)</li>
<li>Ruoyang Xu (undergraduate)</li>
</ul>
<h3 id="former-investigators">Former Investigators</h3>
<ul>
<li>Jin Ha Hwang</li>
</ul>

      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="https://ivalab.github.io/VisNav/public/js/jquery.min.js"></script>
<script src="https://ivalab.github.io/VisNav/public/js/bootstrap.min.js"></script>
<script src="https://ivalab.github.io/VisNav/public/js/jquery.cookie.js"> </script>
<script src="https://ivalab.github.io/VisNav/public/js/ekko-lightbox.js"></script>
<script src="https://ivalab.github.io/VisNav/public/js/jquery.scrollTo.min.js"></script>
<script src="https://ivalab.github.io/VisNav/public/js/masonry.pkgd.min.js"></script>
<script src="https://ivalab.github.io/VisNav/public/js/imagesloaded.pkgd.min.js"></script>
<script src="https://ivalab.github.io/VisNav/public/js/owl.carousel.min.js"></script>
<script src="https://ivalab.github.io/VisNav/public/js/front.js"></script>



</body>
</html>
